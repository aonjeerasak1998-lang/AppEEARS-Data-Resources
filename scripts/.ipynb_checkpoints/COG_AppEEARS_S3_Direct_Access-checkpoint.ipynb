{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece25488-6650-4b31-822e-52a2088ea615",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "\n",
    "This tutorial demonstrates how to access AppEEARS Cloud Optimized GeoTIFF (COG) outputs. NASA's Application for Extracting and Exploring Analysis Ready Samples ([AρρEEARS](https://appeears.earthdatacloud.nasa.gov/)) has been migrated to NASA's Earthdata Cloud space located in **AWS us-west 2**. This enables the user working in the cloud instance deployed in **AWS us-west 2** to access outputs direcly in the cloud using S3 link returned in the location header of the response. In this tutorial, we will walk through the process of submitting an area sample and accessing a Cloud Optimized GeoTIFF (COG) outputs from AppEEARS. \n",
    "The Dixie Fire, the second-largest fire in California history, used as an example in this tutorial. According to [CalFire](https://www.fire.ca.gov/incidents/2021/7/13/dixie-fire/), the fire has started on July 13, 2021 and burned more than 963,276 acres acres. The fire was hundred percent contained by October 2021. On August 18, the Dixie Fire merged with the Morgan Fire, which had been started by lightning August 12, close to Lassen National Park.   \n",
    "\n",
    "**Requirements**    \n",
    "- Earthdata Login Authentication is required to access AppEEARS API and AppEEARS outpurs direcrly from an Amazon AWS bucket. See **Requirements** section in **README.md**.\n",
    "\n",
    "**Learning Objectives**  \n",
    "- Learn how to access AppEEARS Cloud Optimized GeoTIFF (COG) outputs\n",
    "\n",
    "\n",
    "**Tutorial Outline**  \n",
    "   1. Setting Up  \n",
    "   2. Submit an area request in AppEEARS  \n",
    "   3. Extract the Direct S3 links  \n",
    "   4. Create a boto3 Refreshable Session  \n",
    "   5. Single COG File In-Region Direct S3 Access   \n",
    "   6. Multiple COG File In-Region Direct S3 Access  \n",
    "   7. Explore the EVI Time Series   \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d91645-8236-4031-8fb1-7d71ea0f9f8b",
   "metadata": {},
   "source": [
    "## 1. setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3042e1-6301-4ce4-906b-fa70296f99cd",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed631cf3-bc44-4039-98c3-01c56ec544ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass, pprint, time, os, cgi, json\n",
    "import geopandas \n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from urllib import parse\n",
    "import requests\n",
    "from netrc import netrc\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from botocore.client import Config\n",
    "import rioxarray\n",
    "import xarray\n",
    "import hvplot.xarray\n",
    "import holoviews\n",
    "import geoviews\n",
    "import rasterio \n",
    "from rasterio.plot import show\n",
    "from rasterio.session import AWSSession\n",
    "import s3fs\n",
    "import pandas\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('modules/')\n",
    "import aws_session\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2679e1-a79f-4edb-ac71-7a53c3aac62e",
   "metadata": {},
   "source": [
    "In order to successfully run this tutorial, it is required to create a .netrc file in your home directory. The function `_validate_netrc` defined in `aws_session` checks if the netrc file with proper format exists in your home directory. If the netrc file does not exist, it will prompt to ask your Earthdata Login username and password and will create a netrc file. Please see the **Prerequisites** section in `README.md`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ebdb9-a563-4d7d-8053-158193cec4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate if netrc file is present else create one via the user / password prompt for the urs.earthdata.nasa.gov\n",
    "aws_session._validate_netrc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24072e6b-6deb-4e2e-aaca-29422e79aae2",
   "metadata": {},
   "source": [
    "## 2. Submit an area request in AppEEARS \n",
    "In this step, we are going to submit an area request with GeoTIFF as an output format. You can also submit this request using [AppEEARS Graphic User Interface(GUI)](https://appeears.earthdatacloud.nasa.gov/task/area) and upload the JSON file provided in the repository (AppEEARS-Data-Resources/additional_file/Dixie-Fire-request.json). If you have your completed request, save your `task_id` to a variable, skip this step, and move to the next step of tutorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f61879-a31f-4910-b933-d49aae55aac6",
   "metadata": {},
   "source": [
    "Assign the AρρEEARS API endpoint to a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0fe62-090a-4a4e-bd9b-a76324df9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "appeears_API_endpoint = 'https://appeears.earthdatacloud.nasa.gov/api/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a06334-fd80-43c9-8060-658039193773",
   "metadata": {},
   "source": [
    "To access AppEEARS API a **token** is needed. This token is created using AppEEARS API endpoint, Earthdata Login credential stored in .netrc file, and `requests.post` function. This generated token will be added to the header. The header will be passed to `requests.post` function showing you are a qualifid user for submit and access a request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a3399-0dba-4177-a536-d5379445a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "urs = 'urs.earthdata.nasa.gov'\n",
    "token_response = requests.post('{}login'.format(appeears_API_endpoint), auth = (netrc().authenticators(urs)[0],netrc().authenticators(urs)[2])).json() # Insert API URL, call login service, provide credentials & return json \n",
    "token = token_response['token']                      # Save login token to a variable\n",
    "head = {'Authorization': 'Bearer {}'.format(token)}  # Create a header to store token information, needed to submit a request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe9aa0-7b75-4c11-acd2-aacf975b2dd2",
   "metadata": {},
   "source": [
    "Next, compile a JSON object with the request parameters. Dixie fire, started on July 13, 2021, but we extended the search query to two years to see the time series. The GeoJSON of Region of Interest(ROI) including Lassen National Park region, CA can be downloaded from the repository. For this tutorial, we are requesting `_500m_16_days_EVI` layer of `MOD13A1.061` to see how Enhanced Vegetation Indices (EVI) varies before and after the fire event. Learn more about the MODIS Vegetation Indices 16-Day Version 6.1 product [here](https://doi.org/10.5067/MODIS/MOD13A1.061). Below the AppEEARS search parameters are defined and the `task` JSON  .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1a47e-6aef-4bc4-9819-77ec5419c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"Dixie Fire\"\n",
    "task_type = 'area'                  # Type of task, area or point\n",
    "proj = 'geographic'                 # Set output projection \n",
    "outFormat = 'geotiff'               # Set output file format type\n",
    "startDate = '01-01-2021'            # Start of the date range for which to extract data: MM-DD-YYYY\n",
    "endDate = '12-31-2022'              # End of the date range for which to extract data: MM-DD-YYYY\n",
    "ROI =  geopandas.read_file('additional_file/DixieFire.geojson').to_json()\n",
    "prodLayer = [{'layer': '_500m_16_days_EVI', 'product': 'MOD13A1.061'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3df3d-63d2-43f0-a636-4ee5a26f39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'task_type': task_type,\n",
    "    'task_name': task_name,\n",
    "    'params': {\n",
    "         'dates': [\n",
    "         {\n",
    "             'startDate': startDate,\n",
    "             'endDate': endDate\n",
    "         }],\n",
    "         'layers': prodLayer,\n",
    "         'output': {\n",
    "                 'format': {\n",
    "                         'type': outFormat}, \n",
    "                         'projection': proj},\n",
    "         'geo': json.loads(ROI),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc394d-fcf8-4c0f-9e5e-2779c1c824e8",
   "metadata": {},
   "source": [
    "Next, submit the AppEEARS request using `post` function from `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b170a-cb72-48bc-af7d-bc4d4384ef17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_response = requests.post('{}task'.format(appeears_API_endpoint), json=task, headers=head).json()   # Post json to the API task service, return response as json\n",
    "task_response                                                                  # Print task response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27257746-54c5-491c-b044-ec867e8715d9",
   "metadata": {},
   "source": [
    "Save the `task_id` and wait until your request is processed and complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88557fdc-cb11-4cf1-b10c-606f4e2b0a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_id = task_response['task_id']\n",
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0e4ca-d45f-4165-b61c-32f5812f39ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ping API until request is complete, then continue to Section 3\n",
    "while requests.get('{}task/{}'.format(appeears_API_endpoint, task_id), headers=head).json()['status'] != 'done':\n",
    "    print(requests.get('{}task/{}'.format(appeears_API_endpoint, task_id), headers=head).json()['status'])\n",
    "    time.sleep(60)\n",
    "print(requests.get('{}task/{}'.format(appeears_API_endpoint, task_id), headers=head).json()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a553ad8-ae30-4bb8-bd16-189cff4f97c7",
   "metadata": {},
   "source": [
    "## 3. Extract the Direct S3 links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002a29a-1183-4863-ade7-7f9c7b1dc1b4",
   "metadata": {},
   "source": [
    "Now that we have our outputs ready, we can get the bundle information for the files included in the outputs. If you submitted your request using AppEEARS GUI, assign your sample's `task_id` to the variable `task_id` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5cd6a-f31e-497e-9e45-b8db71309164",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 'a1714bef-bb86-4bd3-be54-bb84c01a11d8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd89a2-a979-4ba1-992f-2315c4454ae1",
   "metadata": {},
   "source": [
    "`requests.get` is used toget the bundle information. Below, bundle information for the first output file is printed. The bundle information includes `s3_url` in addition to the other information such as `file_name`, `file_id`, and `file_type`.  \n",
    "Each output file can be downloaded using the `file_id` and AppEEARS API endpoint. AppEEARS outputs are stored in an AWS bucket that can be accessed using `S3_url`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e178306-3e7d-4bd1-a6fc-e59932ab1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = requests.get('{}bundle/{}'.format(appeears_API_endpoint,task_id), headers=head).json()  # Call API and return bundle contents for the task_id as json\n",
    "\n",
    "bundle['files'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddda8db-5888-4b39-81ba-7997f140186b",
   "metadata": {},
   "source": [
    "Below, the S3 Links to Cloud Optimized GeoTIFF outputs and then S3 links for EVI layers are filted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439edfb1-b52b-496d-9fff-3f7e12f63b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_urls = [f['s3_url'] for f in bundle['files'] if f['file_type'] == 'tif']\n",
    "# cog_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0edd6-e306-4102-a760-31cbf2d92fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVI_cog_urls = [link for link in cog_urls if '500m_16_days_EVI' in link]\n",
    "EVI_cog_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1205c4-c047-4289-bac7-0e72969b2705",
   "metadata": {},
   "source": [
    "In the same way, get the links to quality layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633e487-e64d-4670-94cc-27b585ffff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_cog_urls = [link for link in cog_urls if '500m_16_days_VI_Quality' in link]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e0f62-5e88-4007-b3b8-777b48be0a4d",
   "metadata": {},
   "source": [
    "## 4. Create a boto3 Refreshable Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70c4c0-d3b6-41ba-9064-c06499bd10ca",
   "metadata": {},
   "source": [
    "AppEEARS outputs are freely accessible from a cloud instance in `us-west-2` region. In order to access our output files, a **Boto3 session** is needed. The Boto session will stores the required configurations for an easy integration between Python and AWS services. Below, `get_boto3_refreshable_session` stored in `aws_session` will access  your Earthdata login credentidals store in .netrc file and generate S3 credential by making a call to AppEEARS S3 credential endpoint, and create a boto3 session. This session will be auto-renewed as needed to prevent timeouts errors related to S3 credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dde53c-653b-4d2f-a9d4-ebd22ce2dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = 'us-west-2'\n",
    "s3_creds_endpoint = f\"{appeears_API_endpoint}/s3credentials\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8a0f2-99fe-4cc0-b281-be1c71126913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boto3 Session required by the rioxarray package \n",
    "boto3_session = aws_session.get_boto3_refreshable_session(s3_creds_endpoint, region_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a74c93-6e9c-423e-9694-5806c98d0fcb",
   "metadata": {},
   "source": [
    "Below, GDAL and AWS configuration and any environmner options are passed to\n",
    "`rasterio.env()`.  When the Python context manager is entered  all the configuration options are set and when the context is exited, configuration options are removed.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ce394-058f-4749-bca1-d2a06da4601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rio_env = rasterio.Env(\n",
    "    AWSSession(boto3_session),\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "    GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "    GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt')\n",
    ")\n",
    "rio_env.__enter__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268de6c9-a435-4405-b09d-4958569b53a7",
   "metadata": {},
   "source": [
    "## 5. Single COG File In-Region Direct S3 Access "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b694165-3e10-4b5e-b710-034ce4ad32fe",
   "metadata": {},
   "source": [
    "COG datasets can be read using `open_rasterio` function from `rioxarray` library. The coordinates are generated automatically from the file’s geoinformation. \n",
    "Below, the first EVI S3 link is read as a `xarray.DataArray`. The data array is scaled and nodata values are masked by setting `mask_and_scale` to `True`. Chunk sizes can be provided or can be set to 'auto' to make a sensible chunk sized according to each dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadcae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVI_obj = rioxarray.open_rasterio(EVI_cog_urls[0], chunks = 'auto'  , mask_and_scale = True).squeeze('band', drop=True)\n",
    "\n",
    "EVI_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eac0b0-e920-44f2-a889-264034bb10c9",
   "metadata": {},
   "source": [
    "If the chunk shape and array are the same, the chunk selected for the dataset is bigger in size. you can set the chunk size manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a337f2-09bf-4620-9210-1f09090b9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVI_obj.chunk(chunks={\"x\": 5, \"y\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d2a86-09ff-497e-bf02-e28a9cf52be2",
   "metadata": {},
   "source": [
    "To view the values in the data array, you can use `load` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14860f-7beb-40c4-b8e6-f3a843ebea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVI_obj.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ff123-8c86-4777-88fd-07182e776568",
   "metadata": {},
   "source": [
    "Now the data are loaded, lets quickly visualize the first EVI file using `hvplot.image` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc58db2-7ceb-45e2-8d3e-9f605ff933a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVI_obj.hvplot.image(x='x', y='y', rasterize=True, title= EVI_cog_urls[0].split('/')[-1], colorbar=True, cmap='YlGnBu').opts(clim=(0, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859ec71-223f-49f5-a2cb-d3c875f89515",
   "metadata": {},
   "source": [
    "## 6. Multiple COG File In-Region Direct S3 Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610e486-1f76-4eae-a4c6-a2a6c6005c92",
   "metadata": {},
   "source": [
    "Next, lets access the EVI time series. Here, the EVI time series is being read and concadenated to a single `xarray.DataArray` using the `concat` function.  \n",
    "\n",
    "Below the date are saved in a list using S3 links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f66181-fb7c-428b-9b55-fa7380ca1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "for obs in range(len(EVI_cog_urls)):\n",
    "    doy = EVI_cog_urls[obs].split('_doy')[1].split('_aid')[0]\n",
    "    doy_date = datetime.strptime(doy, '%Y%j').strftime('%d-%m-%Y')\n",
    "    time_list.append(doy_date)\n",
    "time = xarray.Variable('time', time_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db2852-8b83-4c11-9bf7-6e0267b9cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=dict(band=1, x=100, y=100)\n",
    "EVI_series = xarray.concat([rioxarray.open_rasterio(f, chunks = 'auto', mask_and_scale = True).squeeze('band', drop=True) for f in EVI_cog_urls], dim=time)\n",
    "EVI_series = EVI_series.rename('EVI')\n",
    "EVI_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50f426-52fc-42a7-8209-aa19a7159190",
   "metadata": {},
   "source": [
    "## 7. Explore the EVI Time Series "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83f005-d171-4b7a-8572-ff4926bb773c",
   "metadata": {},
   "source": [
    "Below, a dynamic plot is created to visually look at the variation in EVI through out these two years. You can manually select the dates from the dropdown next to the map. The latest observation before the fire event is on July 12, 2021. and the next observation is on July 28, 2021 which is the first observation after the fire event. A sudden decrease in EVI after July 12, 2021 is visually noticable and the area that vegetation is being removed grows spatially as you look through next observations. If you look at the observattions of the next year, you still can identify the scar left by fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da759ff6-b85b-4747-95c9-5a26aeaa4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = EVI_series.hvplot.image(x='x', y='y', bands='time', rasterize=True, colorbar=True, cmap='YlGnBu').opts(clim=(0, 0.8))\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f10cf-a505-4854-91be-83e02e17470c",
   "metadata": {},
   "source": [
    "You can also create a plot showing the EVI time series for a specific latittude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bf365-e065-48ea-8939-747b199c8298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVI_series.sel(x=-121.260, y=40.330,method='nearest').hvplot.line(x='time', y='EVI', rot=90, frame_width= 1000, frame_height= 300, fontscale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727873f0-1c2a-4da2-8b5d-b84476532864",
   "metadata": {},
   "source": [
    "Now, combine the time series map and the plot. Below,a dynamic map is created that shows the EVI time series as you move your mouse to a different location on the map. You can update the base map by selecting the date from the dropdown menu on the far right side. This is an easy way to explore your data before further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2e5f9-bed2-4beb-98d5-2409e9ae8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stream of X and Y positional data\n",
    "posxy = holoviews.streams.PointerXY(source=map, x=-121.4, y=40.03) \n",
    "\n",
    "# Function to build a time series using the mouse hover positional information retrieved from the map \n",
    "def point_spectra(x,y):\n",
    "    return EVI_series.sel(x=x,y=y,method='nearest').hvplot.line(x='time', y='EVI', rot=90, frame_width= 800, frame_height= 300, fontscale=1.5) \n",
    "\n",
    "# Define the Dynamic Maps\n",
    "point_dmap = holoviews.DynamicMap(point_spectra, streams=[posxy])\n",
    "\n",
    "# Plot the Map and Dynamic Map side by side\n",
    "map + point_dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d8c12-edb2-49ce-8931-40484f387a05",
   "metadata": {},
   "source": [
    "Finally, exit the context manager to remove the configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3c6ac-3474-4fba-af44-c49350e5ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit our context\n",
    "rio_env.__exit__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee89ee7-96f6-4229-8c33-a6c149441e8b",
   "metadata": {},
   "source": [
    "\n",
    "## Contact Info:  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "Date last modified: 02-16-2022  \n",
    "\n",
    "¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7905ee8-9f5e-4d7b-991a-98d6044daf91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
